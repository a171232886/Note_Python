# LlamaIndex 

å®˜æ–¹æ–‡æ¡£ï¼š[Welcome to LlamaIndex ğŸ¦™ ! | LlamaIndex Python Documentation](https://developers.llamaindex.ai/python/framework/)

# 1. åŸºç¡€ä»‹ç»

## 1.1 æ¦‚å¿µ

1. [What is context augmentation?](https://developers.llamaindex.ai/python/framework/#what-is-context-augmentation)

   > Context augmentation makes your data available to the LLM to solve the problem at hand. LlamaIndex provides the tools to build any of context-augmentation use case, from prototype to production.

2. [LlamaIndex is the framework for Context-Augmented LLM Applications](LlamaIndex is the framework for Context-Augmented LLM Applications)

   ï¼ˆä»…ä¿ç•™å’ŒRAGç›¸å…³çš„ä¸‰ç‚¹ï¼‰

   > - **Data connectors** ingest your existing data from their native source and format. These could be APIs, PDFs, SQL, and (much) more.
   > - **Data indexes** structure your data in intermediate representations that are easy and performant for LLMs to consume.
   > - **Engines** provide natural language access to your data.

3. å…¶å®LlamaIndexå¯ä»¥ç”¨æ¥æ„å»ºAgent

4. [RAGçš„ç®€è¦æ¦‚è¿°](pip install uv)

   > Retrieval-Augmented Generation (RAG) is a core technique for building data-backed LLM applications with LlamaIndex. It allows LLMs to answer questions about your private data by providing it to the LLM at query time, rather than training the LLM on your data. To avoid sending **all** of your data to the LLM every time, RAG indexes your data and selectively sends only the relevant parts along with your query. 

   

   

## 1.2 å¿«é€Ÿå¼€å§‹

1. å®‰è£…

   ```
   pip install llama-index
   ```

   æ³¨æ„ï¼šllama-indexæ¡†æ¶é«˜åº¦æ’ä»¶åŒ–ï¼Œå®˜æ–¹åœ¨ [llamahub](https://llamahub.ai/) ä¸­æä¾›äº†å¤§é‡æ’ä»¶ï¼Œè‹¥ä½¿ç”¨ï¼Œéœ€è¦é¢å¤–å®‰è£…ã€‚æ¯”å¦‚

   ```
   pip install llama-index-llms-openai-like
   ```

   

2. åœ¨`data`æ–‡ä»¶å¤¹ä¸‹å‡†å¤‡å‡ ä¸ªæ–‡ä»¶

   æ¯”å¦‚ï¼š1.txt

   ```
   å…³äºæ•æ·å¼€å‘ï¼ˆAgile Developmentï¼‰ç†è®ºï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸åºå¤§ä¸”ä¸æ–­å‘å±•çš„ä½“ç³»ã€‚å¦‚æœæˆ‘ä»¬è¦æ¢³ç†å®ƒçš„â€œä»£è¡¨ä½œâ€ï¼Œé€šå¸¸å¯ä»¥åˆ†ä¸ºå¥ åŸºç»å…¸ã€æ ¸å¿ƒæ¡†æ¶å’Œå·¥ç¨‹å®è·µä¸‰å¤§ç±»ã€‚
   
   åŸºäºæˆ‘ä¸ºä½ æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œä»¥ä¸‹æ˜¯è¯¥é¢†åŸŸæœ€å…·æƒå¨æ€§å’Œä»£è¡¨æ€§çš„ä¹¦ç±æ¸…å•ï¼š
   
   ğŸ“š 1. å¥ åŸºä¸æ ¸å¿ƒç†è®ºï¼ˆå¿…è¯»ç»å…¸ï¼‰
   å¦‚æœä½ æƒ³ç†è§£æ•æ·çš„â€œä¸ºä»€ä¹ˆâ€å’Œâ€œæ˜¯ä»€ä¹ˆâ€ï¼Œè¿™ä¸¤æœ¬ä¹¦æ˜¯ç»•ä¸å¼€çš„åŸºçŸ³ï¼š
   
   ã€Šæ•æ·è½¯ä»¶å¼€å‘ï¼šåŸåˆ™ã€æ¨¡å¼ä¸å®è·µã€‹ (Agile Software Development: Principles, Patterns, and Practices)
   ä½œè€…ï¼š Robert C. Martin (Uncle Bob)
   åœ°ä½ï¼š è¢«èª‰ä¸ºæ•æ·å¼€å‘çš„â€œåœ£ç»â€ã€‚è¿™æœ¬ä¹¦ä¸ä»…é˜è¿°äº†æ•æ·çš„åŸåˆ™ï¼Œè¿˜ç»“åˆäº†é¢å‘å¯¹è±¡è®¾è®¡çš„åŸåˆ™ï¼ˆSOLIDåŸåˆ™ï¼‰ã€‚
   æ ¸å¿ƒå†…å®¹ï¼š å®ƒæ·±å…¥æ¢è®¨äº†æ•æ·å¼€å‘çš„æ ¸å¿ƒç†å¿µã€è®¾è®¡æ¨¡å¼ä»¥åŠå®é™…ç¼–ç¨‹è§„èŒƒï¼Œå¼ºè°ƒäº†ä»£ç çš„æ•´æ´å’Œå¯ç»´æŠ¤æ€§ã€‚å¯¹äºå¼€å‘è€…å’Œç®¡ç†è€…æ¥è¯´ï¼Œè¿™æ˜¯ç†è§£æ•æ·å…¨è²Œçš„æœ€ä½³èµ·ç‚¹ã€‚
   ã€Šæ•æ·å®£è¨€ã€‹ (Agile Manifesto)
   æ³¨æ„ï¼š è™½ç„¶å®ƒä¸æ˜¯ä¸€æœ¬ä¹¦ï¼Œä½†å®ƒæ˜¯æ‰€æœ‰æ•æ·ç†è®ºçš„æºå¤´ã€‚ç”±Kent Beckç­‰17ä½è½¯ä»¶å¼€å‘é¢†å†›äººç‰©äº2001å¹´ç­¾ç½²ï¼Œç¡®ç«‹äº†â€œä¸ªä½“å’Œäº’åŠ¨ã€å¯å·¥ä½œçš„è½¯ä»¶ã€å®¢æˆ·åˆä½œã€å“åº”å˜åŒ–â€å››å¤§ä»·å€¼è§‚ã€‚
   ğŸ—ï¸ 2. ä¸»æµæ¡†æ¶è¯¦è§£ï¼ˆScrumä¸çœ‹æ¿ï¼‰
   åœ¨å®é™…æ“ä½œä¸­ï¼Œå¤§å¤šæ•°å›¢é˜Ÿä½¿ç”¨çš„æ˜¯ Scrum æˆ– çœ‹æ¿ã€‚ä»¥ä¸‹æ˜¯è¿™ä¸¤ä¸ªé¢†åŸŸçš„æƒå¨è‘—ä½œï¼š
   ...
   ```

   

3. æµ‹è¯•ä»£ç 

   ```python
   # E1_quickstart.py
   from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings
   from llama_index.llms.openai_like import OpenAILike
   from llama_index.embeddings.openai_like import OpenAILikeEmbedding
   
   Settings.embed_model = OpenAILikeEmbedding(
       model_name="text-embedding-v4",
       api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
       api_key="sk-2020f984b4cf4cf49db7668fd5b597d0"
   )
   Settings.llm = OpenAILike(
       model="qwen3-next-80b-a3b-instruct",
       api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
       api_key="sk-2020f984b4cf4cf49db7668fd5b597d0",
       is_chat_model=True
   )
   
   
   # 1. åŠ è½½æ•°æ®
   documents = SimpleDirectoryReader("data/").load_data()
   
   # 2. æ„å»ºç´¢å¼•
   index = VectorStoreIndex.from_documents(documents)
   
   # 3. åˆ›å»ºæŸ¥è¯¢å¼•æ“
   query_engine = index.as_query_engine()
   # index.as_retriever()
   
   # 4. æé—®
   response = query_engine.query("åº”è¯¥æ¨èé‚£ä¸¤æœ¬ä¹¦ï¼Ÿ")
   print(response)
   ```
   è¾“å‡º
   ```
   ã€Šæ•æ·è½¯ä»¶å¼€å‘ï¼šåŸåˆ™ã€æ¨¡å¼ä¸å®è·µã€‹å’Œã€ŠScrumæŒ‡å—ã€‹
   ```




# 2. RAGä»‹ç»

## 2.1 ç†è®º

ï¼ˆæ ¹æ®[Introduction to RAG](https://developers.llamaindex.ai/python/framework/understanding/rag/)ç²¾ç®€å¾—æ¥ï¼‰

1. èƒŒæ™¯

   > LLMs are trained on enormous bodies of data but they arenâ€™t trained on **your** data. Retrieval-Augmented Generation (RAG) solves this problem by adding your data to the data LLMs already have access to.

2. æ­¥éª¤æ¦‚è§ˆ

   > - In RAG, your data is loaded and prepared for queries or â€œindexedâ€. 
   >
   > - User queries act on the index, which filters your data down to the most relevant context. 
   >
   > - This context and your query then go to the LLM along with a prompt, and the LLM provides a response.

   ![img](images/LlamaIndex/basic_rag.sdlwNwWz_Z1yQWLG.png)

3. æ ¸å¿ƒæ­¥éª¤

   ![img](images/LlamaIndex/stages.B-QMnT9I_1uEetk.png)

   - **Loading**: this refers to getting your data from where it lives â€” whether itâ€™s text files, PDFs, another website, a database, or an API â€” into your workflow.
     -  [LlamaHub](https://llamahub.ai/) provides hundreds of connectors to choose from.
     - æ³¨æ„ RAG æ ¸å¿ƒçš„â€œæ•°æ®åˆ†æ®µâ€æŠ€æœ¯ï¼Œåœ¨LlamaIndexä¸­è¢«åˆ’åˆ†è‡³â€œæ•°æ®è½½å…¥â€
   - **Indexing**: this means creating a data structure that allows for querying the data. 
     - For LLMs this nearly always means creating `vector embeddings`, **numerical representations of the meaning of your data**, as well as numerous other metadata strategies to make it easy to accurately find contextually relevant data.
   - **Storing**: once your data is indexed you will almost always want to store your index, as well as other metadata, to avoid having to re-index it.
     - æ•°æ®ä¸ç´¢å¼•å­˜å‚¨ç›¸å…³ï¼Œä¹Ÿæ˜¯LlamaIndexæ¡†æ¶ä¸‹æœ€éœ€è¦ç”¨æˆ·å…³æ³¨çš„åœ°æ–¹
   - **Querying**: for any given indexing strategy there are many ways you can utilize LLMs and LlamaIndex data structures to query, including sub-queries, multi-step queries and hybrid strategies.
     - å…ˆä»æ•°æ®åº“ä¸­å–å›é—®é¢˜ç›¸å…³èµ„æ–™ï¼Œç„¶åå†é€åˆ°LLMä¸­
   - **Evaluation**: a critical step in any flow is checking how effective it is relative to other strategies, or when you make changes. Evaluation provides objective measures of how accurate, faithful and fast your responses to queries are.



## 2.2 å…³é”®å®ä½“

1. [**Nodes and Documents**](https://developers.llamaindex.ai/python/framework/module_guides/loading/documents_and_nodes): 

   > - A `Document` is a container around any data source - for instance, a PDF, an API output, or retrieve data from a database. 
   > - **A `Node` is the atomic unit of data in LlamaIndex and represents a â€œchunkâ€ of a source `Document`. **
   > - Nodes have metadata that relate them to the document they are in and to other nodes.

   å³ä¸€ä»½æ–‡ä»¶å¯¹åº”çš„æ˜¯Documentï¼Œä¼šè¢«æ‹†åˆ†æˆå¤šä¸ªNode

2. [**Embeddings**](https://developers.llamaindex.ai/python/framework/module_guides/models/embeddings): 

   > LLMs generate numerical representations of data called `embeddings`. When filtering your data for relevance, LlamaIndex will convert queries into embeddings.

3. [**Indexes**](https://developers.llamaindex.ai/python/framework/module_guides/indexing):
   - å½“nodeåŠå…¶å¯¹åº”çš„embeddingæ„å»ºç»“æŸåï¼Œé€‰æ‹©ä¸€éƒ¨åˆ†ï¼ˆæˆ–å…¨éƒ¨ï¼‰nodeä½œä¸ºä¸€ä¸ªæ•´ä½“é›†åˆï¼Œæˆä¸ºindex
   - æœªæ¥è¿›è¡Œæ£€ç´¢æ—¶ï¼Œé€‰æ‹©åœ¨å“ªä¸ªé›†åˆï¼Œä¹Ÿå°±æ˜¯åœ¨å“ªä¸ªindexä¸­æ£€ç´¢

4. [**Retrievers**](https://developers.llamaindex.ai/python/framework/module_guides/querying/retriever):

   > A retriever defines how to efficiently retrieve relevant context from an index when given a query. 

   æ ¹æ®é—®é¢˜ï¼Œä»æ•°æ®åº“ä¸­è·å–ç›¸å…³node

5. [**Node Postprocessors**](https://developers.llamaindex.ai/python/framework/module_guides/querying/node_postprocessors): A node postprocessor takes in a set of retrieved nodes and applies transformations, filtering, or re-ranking logic to them.
   - è¾“å…¥æ˜¯nodeï¼Œè¾“å‡ºä¹Ÿæ˜¯node
   - å¯¹nodeçš„å¤„ç†æ“ä½œé›†åˆ

6. åœ¨å¤æ‚æƒ…å†µä¸‹ï¼ˆå¤šä¸ªRetrieversï¼‰ï¼Œè¿˜æœ‰[**Routers**](https://developers.llamaindex.ai/python/framework/module_guides/querying/router)ç”¨äºç¡®å®šé€‰æ‹©å“ªä¸ªRetrievers





# 3. Loading

## 3.1 æ•°æ®ç»“æ„

### 3.1.1 Document

ï¼ˆç»“åˆ[Documents / Nodes](https://developers.llamaindex.ai/python/framework/module_guides/loading/documents_and_nodes/)ä¸[Defining and Customizing Documents](https://developers.llamaindex.ai/python/framework/module_guides/loading/documents_and_nodes/usage_documents/)ï¼‰

1. A **Document** is a generic container around any data source 

   for instance, a PDF, an API output, or retrieved data from a database. 

   - They can be constructed manually, 

     ```python
     from llama_index.core import Document, VectorStoreIndex
     
     text_list = [text1, text2, ...]
     documents = [Document(text=t) for t in text_list]
     ```

     

   - or created automatically via our data loaders.

     ```python
     from llama_index.core import SimpleDirectoryReader
     documents = SimpleDirectoryReader("./data").load_data()
     ```

     

2. Document stores text along with some other attributes

   - `metadata` - a dictionary of annotations that can be appended to the text.
     - This information can be anything, such as filenames or categories.
     -  If you are integrating with a vector database, keep in mind that some vector databases require that the keys must be strings, and the values must be flat (either `str`, `float`, or `int`).
   - `relationships` - a dictionary containing relationships to other Documents/Nodes.



3. æ¡†æ¶æä¾›äº†ä¸€ä¸ªé»˜è®¤Documentç”¨äºç ”ç©¶å’ŒåŸå‹å¼€å‘

   ```
   document = Document.example()
   ```

   ```json
   {
       "id_": "a27b5ad3-4430-4b7e-a247-da8af9fd034e",
       "embedding": null,
       "metadata": {
           "filename": "README.md",
           "category": "codebase"
       },
       "excluded_embed_metadata_keys": [],
       "excluded_llm_metadata_keys": [],
       "relationships": {},
       "metadata_template": "{key}: {value}",
       "metadata_separator": "\n",
       "text_resource": {
           "embeddings": null,
           "text": "\nContext\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\nThey are pre-trained on large amounts of publicly available data.\nHow do we best augment LLMs with our own private data?\nWe need a comprehensive toolkit to help perform this data augmentation for LLMs.\n\nProposed Solution\nThat's where LlamaIndex comes in. LlamaIndex is a \"data framework\" to help\nyou build LLM  apps. It provides the following tools:\n\nOffers data connectors to ingest your existing data sources and data formats\n(APIs, PDFs, docs, SQL, etc.)\nProvides ways to structure your data (indices, graphs) so that this data can be\neasily used with LLMs.\nProvides an advanced retrieval/query interface over your data:\nFeed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\nAllows easy integrations with your outer application framework\n(e.g. with LangChain, Flask, Docker, ChatGPT, anything else).\nLlamaIndex provides tools for both beginner users and advanced users.\nOur high-level API allows beginner users to use LlamaIndex to ingest and\nquery their data in 5 lines of code. Our lower-level APIs allow advanced users to\ncustomize and extend any module (data connectors, indices, retrievers, query engines,\nreranking modules), to fit their needs.\n",
           "path": null,
           "url": null,
           "mimetype": null
       },
       "image_resource": null,
       "audio_resource": null,
       "video_resource": null,
       "text_template": "{metadata_str}\n\n{content}",
       "class_name": "Document",
       "text": "\nContext\nLLMs are a phenomenal piece of technology for knowledge generation and reasoning.\nThey are pre-trained on large amounts of publicly available data.\nHow do we best augment LLMs with our own private data?\nWe need a comprehensive toolkit to help perform this data augmentation for LLMs.\n\nProposed Solution\nThat's where LlamaIndex comes in. LlamaIndex is a \"data framework\" to help\nyou build LLM  apps. It provides the following tools:\n\nOffers data connectors to ingest your existing data sources and data formats\n(APIs, PDFs, docs, SQL, etc.)\nProvides ways to structure your data (indices, graphs) so that this data can be\neasily used with LLMs.\nProvides an advanced retrieval/query interface over your data:\nFeed in any LLM input prompt, get back retrieved context and knowledge-augmented output.\nAllows easy integrations with your outer application framework\n(e.g. with LangChain, Flask, Docker, ChatGPT, anything else).\nLlamaIndex provides tools for both beginner users and advanced users.\nOur high-level API allows beginner users to use LlamaIndex to ingest and\nquery their data in 5 lines of code. Our lower-level APIs allow advanced users to\ncustomize and extend any module (data connectors, indices, retrievers, query engines,\nreranking modules), to fit their needs.\n"
   }
   ```

   

4. doc_id å¯é€šè¿‡å±æ€§ä¿®æ”¹

   ```
   document.doc_id = "My new document id!"
   ```
   
   
   
5. metadataçš„è‡ªå®šä¹‰

   ```python
   # æ–¹æ³•ä¸€
   document = Document(
       text="text",
       metadata={"filename": "<doc_file_name>", "category": "<category>"},
   )
   
   # æ–¹æ³•äºŒ
   document.metadata = {"filename": "<doc_file_name>"}
   ```



6. metadataçš„é«˜çº§è®¾ç½®

   å¤§æ¨¡å‹çš„ä¸Šä¸‹æ–‡é•¿åº¦æœ‰é™åˆ¶ï¼Œéå¿…è¦çš„æ— å…³ä¿¡æ¯åº”è¯¥ä¸ä¼ å…¥

   ```python
   # å‘ embedding model ä¼ å…¥promptæ—¶ï¼Œå¿½ç•¥å“ªäº›å­—æ®µ
   document.excluded_embed_metadata_keys = ["file_name"]
   
   # å‘å¯¹è¯ llm ä¼ å…¥promptæ—¶ï¼Œå¿½ç•¥å“ªäº›å­—æ®µ
   document.excluded_llm_metadata_keys = ["file_name"]
   ```

   

### 3.1.2 Node

ï¼ˆç»“åˆ[Documents / Nodes](https://developers.llamaindex.ai/python/framework/module_guides/loading/documents_and_nodes/)ä¸[Defining and Customizing Nodes](https://developers.llamaindex.ai/python/framework/module_guides/loading/documents_and_nodes/usage_nodes/)ï¼‰

1. A **Node** represents a â€œchunkâ€ of a source Document, whether that is a text chunk, an image, or other. 

   - contain metadata and relationship information with other nodes.

   

2. ä¸¤ç§æ„å»ºæ–¹å¼

   - é€šè¿‡å¯¹Documentçš„åˆ‡åˆ†

     ```python
     from llama_index.core.node_parser import SentenceSplitter
     document = Document.example()
     parser = SentenceSplitter(chunk_size=10, chunk_overlap=1)
     nodes = parser.get_nodes_from_documents(documents=[document])
     ```

     

   - è‡ªè¡Œåˆ›å»º

     ```python
     from llama_index.core.schema import TextNode
     node1 = TextNode(text="<text_chunk>", id_="<node_id>")
     ```

     

3. Nodes are a first-class citizen in LlamaIndex.

   - Document æ˜¯ Node çš„å­ç±»ï¼Œç¡®åˆ‡çš„è¯´æ˜¯ TextNode çš„å­ç±»

   

4. node_id å¯é€šè¿‡å±æ€§ä¿®æ”¹

   ```
   node.node_id = "My new node_id!"
   ```

   

5. By default **every Node derived from a Document will inherit the same metadata from that Document** (e.g. a â€œfile_nameâ€ filed in the Document is propagated to every Node).



6. nodeçš„å…¶ä»–å±æ€§

   - `ref_doc_id`æŒ‡å½“å‰nodeæ˜¯å“ªä¸ªdocumentçš„â€œç‰‡æ®µâ€
   - `relationships`åŒ…å«ä¸¤éƒ¨åˆ†
     - SOURCE Nodeï¼ˆdocumentï¼‰
     - Next Node
   
   - Prevous Node å’Œ Next Node
   
   ```python
   from llama_index.core import Document
   from llama_index.core.schema import TextNode
   from llama_index.core.node_parser import SentenceSplitter
   
   def the_node():
       long_text = """è¿™æ˜¯ç¬¬ä¸€å¥ã€‚è¿™æ˜¯ç¬¬äºŒå¥ã€‚è¿™æ˜¯ç¬¬ä¸‰å¥ã€‚è¿™æ˜¯ç¬¬å››å¥ã€‚è¿™æ˜¯ç¬¬äº”å¥ã€‚è¿™æ˜¯ç¬¬å…­å¥ã€‚è¿™æ˜¯ç¬¬ä¸ƒå¥ã€‚è¿™æ˜¯ç¬¬å…«å¥ã€‚è¿™æ˜¯ç¬¬ä¹å¥ã€‚è¿™æ˜¯ç¬¬åå¥ã€‚"""
       document = Document(text=long_text)
       parser = SentenceSplitter(chunk_size=30, chunk_overlap=10)
       # document = Document.example()
       # parser = SentenceSplitter(chunk_size=100, chunk_overlap=10)
   
       nodes = parser.get_nodes_from_documents(documents=[document])
       for node in nodes:
           print(node.text)
           print("===============================")
   
       print("Node ID: ", nodes[1].node_id)
       print("Node Relationships: ", nodes[1].relationships)
       
       print("Pre Node: ", nodes[1].prev_node)   # è‡³å°‘æ˜¯åŒå‘é“¾è¡¨
       print("Next Node: ",nodes[1].next_node)
   
   
       print(nodes[0].ref_doc_id == document.doc_id)       # True
       print(nodes[0].metadata == document.metadata)       # True
       print(nodes[0].ref_doc_id == nodes[1].ref_doc_id)   # True
   
   
       # å¯ä»¥å•ç‹¬åˆ›å»º
       own_node = TextNode(text="hello")
   
   the_node()
   ```
   
   è¾“å‡º
   
   Nodeåˆ’åˆ†æƒ…å†µ
   
   ```
   è¿™æ˜¯ç¬¬ä¸€å¥ã€‚è¿™æ˜¯ç¬¬äºŒå¥ã€‚è¿™æ˜¯ç¬¬ä¸‰å¥ã€‚è¿™æ˜¯ç¬¬å››å¥ã€‚
   ===============================
   è¿™æ˜¯ç¬¬å››å¥ã€‚è¿™æ˜¯ç¬¬äº”å¥ã€‚è¿™æ˜¯ç¬¬å…­å¥ã€‚è¿™æ˜¯ç¬¬ä¸ƒå¥ã€‚
   ===============================
   è¿™æ˜¯ç¬¬ä¸ƒå¥ã€‚è¿™æ˜¯ç¬¬å…«å¥ã€‚è¿™æ˜¯ç¬¬ä¹å¥ã€‚
   ===============================
   è¿™æ˜¯ç¬¬ä¹å¥ã€‚è¿™æ˜¯ç¬¬åå¥ã€‚
   ===============================
   ```
   
   Node ä¿¡æ¯æ‰“å°
   
   ```bash
   Node ID:  ec2de07d-3371-400a-b971-e2d0f24925c1
   
   Node Relationships:  {
   <NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='3a8eca19-6d19-4684-b53e-d37033a039d3', node_type=<ObjectType.DOCUMENT: '4'>, metadata={}, hash='5e7a226a0749dd110026172245ef35f96a9eccd132eae1bc3dfb68c88a8b2f5b'), 
   
   <NodeRelationship.NEXT: '3'>: RelatedNodeInfo(node_id='bd6690aa-7e09-49be-9f56-96479e57068f', node_type=<ObjectType.TEXT: '1'>, metadata={}, hash='3e785a9de7d7253b745c631969e56ede00121cfdaf8977d0c99f1691aeda5210')
   }
   
   Pre Node:  node_id='ec2de07d-3371-400a-b971-e2d0f24925c1' node_type=<ObjectType.TEXT: '1'> metadata={} hash='a73ed50709b2032bcae6d76dda92824b3403c75e99c9bcbacb807c2ee57446d8'
   
   Next Node:  node_id='a5a08a31-6e45-47d8-9605-35c91fbea1b3' node_type=<ObjectType.TEXT: '1'> metadata={} hash='8288ce88e1bce68a850cdcf6eae89e445335694eb72198b1758a24ff51d44d19'
   ```



## 3.2 SimpleDirectoryReader

Readerä¸ºå®ç°å¯¹ä¸åŒæ•°æ®æ¥æºçš„æ•°æ®è½½å…¥ï¼Œè¯¦ç»†åˆ†ç±»è§[Module Guides](https://developers.llamaindex.ai/python/framework/module_guides/loading/connector/modules/)

- æ­¤å¤„åªä»‹ç»æœ€å¸¸ç”¨çš„[SimpleDirectoryReader](https://developers.llamaindex.ai/python/framework/module_guides/loading/simpledirectoryreader/)

æ³¨æ„`Reader.load_data()`çš„è¾“å‡ºæ˜¯`list[Document]`



SimpleDirectoryReaderå¯é’ˆå¯¹å¤šç§æ–‡ä»¶ç±»å‹

- .csv - comma-separated values
- .docx - Microsoft Word
- .epub - EPUB ebook format
- .hwp - Hangul Word Processor
- .ipynb - Jupyter Notebook
- .jpeg, .jpg - JPEG image
- .mbox - MBOX email archive
- .md - Markdown
- .mp3, .mp4 - audio and video
- .pdf - Portable Document Format
- .png - Portable Network Graphics
- .ppt, .pptm, .pptx - Microsoft PowerPoint




### 3.2.1 è¯»å–

1. åŸºç¡€ä½¿ç”¨ï¼šè¯»å–æœ¬åœ°æ–‡ä»¶

   ```python
   from llama_index.core import SimpleDirectoryReader
   
   reader = SimpleDirectoryReader(
       input_dir="path/to/directory", 
       recursive=True
   )
   documents = reader.load_data()
   ```

   æ³¨æ„`recursive=True`ï¼Œå¦åˆ™åªå¯¼å…¥é¡¶å±‚æ–‡ä»¶

   

2. é™åˆ¶æ¨¡å¼

   ```python
   # æŒ‡å®šæ–‡ä»¶
   SimpleDirectoryReader(input_files=["path/to/file1", "path/to/file2"])
   
   # æ’é™¤æ–‡ä»¶
   SimpleDirectoryReader(
       input_dir="path/to/directory", exclude=["path/to/file1", "path/to/file2"]
   )
   
   # æŒ‡å®šç±»å‹
   SimpleDirectoryReader(
       input_dir="path/to/directory", required_exts=[".pdf", ".docx"]
   )
   ```

   

3. å¯ä»¥ç”¨äºç›´æ¥è¯»å–æ”¯æŒå¤šç§åè®®çš„äº‘ç›˜

   > This can be any filesystem object that is implemented by the [`fsspec`](https://filesystem-spec.readthedocs.io/en/latest/) protocol. The `fsspec` protocol has open-source implementations for a variety of remote filesystems including [AWS S3](https://github.com/fsspec/s3fs), [Azure Blob & DataLake](https://github.com/fsspec/adlfs), [Google Drive](https://github.com/fsspec/gdrivefs), [SFTP](https://github.com/fsspec/sshfs), and [many others](https://github.com/fsspec/).

   ä»¥æœ€ä¸ºå¸¸ç”¨çš„`AWS S3`ä¸ºä¾‹

   ä½¿ç”¨docker compose å¯åŠ¨
   
   ```yaml
   services:
     minio:
       image: minio/minio:latest
       container_name: minio
       ports:
         - "9000:9000"  # APIç«¯å£
         - "9001:9001"  # æ§åˆ¶å°ç«¯å£
       environment:
         MINIO_ROOT_USER: minioadmin
         MINIO_ROOT_PASSWORD: minioadmin
       volumes:
         - ./db/minio_data:/data
       command: server /data --console-address ":9001"
   
   ```
   
   
   
   ç¼–å†™è„šæœ¬
   
   ```python
   from s3fs import S3FileSystem
   from llama_index.core import SimpleDirectoryReader
   
   fs = S3FileSystem(
       key="minioadmin",
       secret="minioadmin",
       client_kwargs={
           "endpoint_url": "http://192.168.0.100:9000",
           "region_name": "cn-north-1",
           "verify": False,
           "use_ssl": False
       },
       config_kwargs={"s3": {"addressing_style": "path"}}
   )
   
   
   reader = SimpleDirectoryReader(
       input_dir="rag",            # bucket name
       fs=fs,
       recursive=True
   )
   
   documents = reader.load_data()
   
   print(documents)
   ```



### 3.2.2 metadata

1. é»˜è®¤æå–é¡¹

   > `SimpleDirectoryReader` will automatically attach a `metadata` dictionary to each `Document` object. By default, this dictionary has these items:
   >
   > - `file_path`: the full filesystem path to the file, including the file name (string)
   > - `file_name`: the file name, including suffix (string)
   > - `file_type`: the MIME type of the file, as guessed by [`mimetypes.guess_type()](https://docs.python.org/3/library/mimetypes.html#mimetypes.guess_type) (string)
   > - `file_size`: the size of the file, in bytes (integer)
   > - `creation_date`, `last_modified_date`, `last_accessed_date`: the creation, modification, and access dates for the file, normalized to the UTC timezone. See [Date and time metadata](https://developers.llamaindex.ai/python/framework/module_guides/loading/simpledirectoryreader/#date-and-time-metadata) below (string)

   

2. å®šåˆ¶ç”Ÿæˆçš„Documentçš„metadata

   ```python
   from llama_index.core import SimpleDirectoryReader
   
   # è®¾ç½®å‡½æ•°
   filename_fn = lambda filename: {"file_name": filename}
   
   # automatically sets the metadata of each document according to filename_fn
   documents = SimpleDirectoryReader(
       "./data", file_metadata=filename_fn
   ).load_data()
   ```




3. æ¡†æ¶é›†æˆäº†ä¸€äº›å…ƒä¿¡æ¯æå–å™¨ï¼ˆ[metadata extractor](https://developers.llamaindex.ai/python/framework/module_guides/loading/documents_and_nodes/usage_metadata_extractor/)ï¼‰
   - `SummaryExtractor` - automatically extracts a summary over a set of Nodes
   - `QuestionsAnsweredExtractor` - extracts a set of questions that each Node can answer
   - `TitleExtractor` - extracts a title over the context of each Node
   - `EntityExtractor` - extracts entities (i.e. names of places, people, things) mentioned in the content of each Node



## 3.3 Node Parser

### 3.3.1 ä»‹ç»

1. Node Parserï¼šå°† `Document` åˆ†æˆ `Node`

   > Node parsers are a simple abstraction that take a list of documents, and **chunk them into `Node` objects**, such that each node is a specific chunk of the parent document.

2. åŸºç¡€ä½¿ç”¨

   ```python
   from llama_index.core import Document
   from llama_index.core.node_parser import SentenceSplitter
   
   node_parser = SentenceSplitter(chunk_size=1024, chunk_overlap=20)
   
   nodes = node_parser.get_nodes_from_documents(
       [Document(text="long text")], show_progress=False
   )
   ```

   

### 3.3.2 ç±»å‹

ï¼ˆ[Node Parser Modules | LlamaIndex Python Documentation](https://developers.llamaindex.ai/python/framework/module_guides/loading/node_parsers/modules/)ç®€åŒ–ï¼‰

1. [File-Based](https://developers.llamaindex.ai/python/framework/module_guides/loading/node_parsers/modules/#file-based-node-parsers)ç±»

   - `HTMLNodeParser`
   - `JSONNodeParser`
   - `MarkdownNodeParser`

   æ¨èä½¿ç”¨ï¼š`SimpleFileNodeParser`è‡ªåŠ¨åˆ¤åˆ«æ•°æ®ç±»å‹é€‰å–åˆé€‚çš„Node Parser

2. [Text-Splitters](https://developers.llamaindex.ai/python/framework/module_guides/loading/node_parsers/modules/#text-splitters) ç±»

   - `CodeSplitter`

   - `SentenceSplitter`ï¼šæ ¹æ®å•è¯æ•°é‡åˆ’åˆ†å—ï¼Œæœ€å¸¸ç”¨

     ```python
         chunk_size: int = Field(
             default=DEFAULT_CHUNK_SIZE,
             description="The token chunk size for each chunk.",			# æ¯å—å¤§å°
             gt=0,
         )
         chunk_overlap: int = Field(
             default=SENTENCE_CHUNK_OVERLAP,
             description="The token overlap of each chunk when splitting.",	# å—ä¹‹é—´çš„é‡å¤
             ge=0,
         )
         # åˆ’åˆ†é‡å¤éƒ¨åˆ†æ—¶ï¼Œå¥å­çš„å®Œæ•´åº¦ ä¼˜å…ˆ æŒ‡å®šoverlapæ•°é‡
     ```

     ä½¿ç”¨

     ```python
     from llama_index.core.node_parser import SentenceSplitter
     
     splitter = SentenceSplitter(
         chunk_size=1024,	# æ¯å—å¤§å°
         chunk_overlap=20,	# å—ä¹‹é—´çš„é‡å¤
     )
     nodes = splitter.get_nodes_from_documents(documents)
     ```
   
     
   
   - `TokenTextSplitter`ï¼šä»¥tokenæ•°é‡ä¸ºå•ä½è¿›è¡Œåˆ’åˆ†

### 3.3.3 é«˜çº§ä½¿ç”¨

1. åœ¨æ„å»ºç´¢å¼•æ—¶ä½¿ç”¨

   ```python
   from llama_index.core import SimpleDirectoryReader, VectorStoreIndex
   from llama_index.core.node_parser import SentenceSplitter
   
   documents = SimpleDirectoryReader("./data").load_data()
   
   # global
   from llama_index.core import Settings
   
   Settings.text_splitter = SentenceSplitter(chunk_size=1024, chunk_overlap=20)
   
   # per-index
   index = VectorStoreIndex.from_documents(
       documents,
       transformations=[SentenceSplitter(chunk_size=1024, chunk_overlap=20)],
   )
   ```

   

2. åœ¨IngestionPipelineä¸­ä½¿ç”¨

   ```python
   from llama_index.core import SimpleDirectoryReader
   from llama_index.core.ingestion import IngestionPipeline
   from llama_index.core.node_parser import TokenTextSplitter
   
   documents = SimpleDirectoryReader("./data").load_data()
   
   pipeline = IngestionPipeline(transformations=[TokenTextSplitter(), ...])
   
   nodes = pipeline.run(documents=documents)
   ```

   

## 3.4 Ingestion Pipeline

### 3.4.1 Transformations

1. A transformation is something that takes a list of **`Node` as an input, and returns a list of Node.**

   ä¾‹å¦‚, 

   - [`TextSplitter`](https://developers.llamaindex.ai/python/framework/module_guides/loading/node_parsers/modules#text-splitters)
   - [`NodeParser`](https://developers.llamaindex.ai/python/framework/module_guides/loading/node_parsers/modules)
   - [`MetadataExtractor`](https://developers.llamaindex.ai/python/framework/module_guides/loading/documents_and_nodes/usage_metadata_extractor)
   - `Embeddings`model (check our [list of supported embeddings](https://developers.llamaindex.ai/python/framework/module_guides/models/embeddings#list-of-supported-embeddings))

   

2. Each component that implements the `Transformation` base class has both a synchronous `__call__()` definition and an async `acall()` definition.



3. ä½¿ç”¨æ–¹å¼

   ```python
   from llama_index.core import VectorStoreIndex
   from llama_index.core.extractors import (
       TitleExtractor,
       QuestionsAnsweredExtractor,
   )
   from llama_index.core.ingestion import IngestionPipeline
   from llama_index.core.node_parser import TokenTextSplitter
   
   transformations = [
       TokenTextSplitter(chunk_size=512, chunk_overlap=128),
       TitleExtractor(nodes=5),
       QuestionsAnsweredExtractor(questions=3),
   ]
   
   index = VectorStoreIndex.from_documents(
       documents, transformations=transformations
   )
   ```



### 3.4.2 Pipeline

1. An `IngestionPipeline` uses a concept of `Transformations` that are applied to input data.

2. ä½¿ç”¨æ–¹å¼

   ```python
   client = qdrant_client.QdrantClient(location=":memory:")
   vector_store = QdrantVectorStore(client=client, collection_name="test_store")
   
   pipeline = IngestionPipeline(
       transformations=[
           SentenceSplitter(chunk_size=25, chunk_overlap=0),
           TitleExtractor(),
           OpenAIEmbedding(),
       ],
       vector_store=vector_store,
   )
   
   # Ingest directly into a vector db
   pipeline.run(documents=[Document.example()])
   
   # Create your index
   from llama_index.core import VectorStoreIndex
   
   index = VectorStoreIndex.from_vector_store(vector_store)
   ```

3. æä¾›cacheçš„å­˜å‚¨ä¸åŠ è½½åŠŸèƒ½

   ï¼ˆå­˜åœ¨äº›è®¸bugï¼‰

4. æ”¯æŒå¼‚æ­¥
5. ä¸ªäººä¸å»ºè®®ä½¿ç”¨pipelineï¼Œæ¡†æ¶å„æ¨¡å—å·²ç»é«˜åº¦é›†æˆåŒ–ï¼Œå‡ä¸ºæ‰‹åŠ¨å®šä¹‰æµç¨‹æ›´åˆé€‚





# 4. Storing

1. LlamaIndex also supports swappable **storage components** that allows you to customize:

   - **Document stores**: **where ingested documents (i.e., `Node` objects) are stored**,
   - **Index stores**: where index metadata are stored,
   - **Vector stores**: where embedding vectors are stored.
   - **Property Graph stores**: where knowledge graphs are stored (i.e. for `PropertyGraphIndex`).
   - **Chat Stores**: where chat messages are stored and organized.

   The Document/Index stores rely on a common Key-Value store abstraction, which is also detailed below.

   æ³¨æ„ï¼š

   - **Document storesä¿å­˜çš„å¹¶ä¸æ˜¯æ–‡ä»¶æœ¬èº«ï¼Œæ˜¯nodeï¼Œä»¥åŠnodeå’Œdocä¹‹é—´çš„å…³ç³»**
   - æ ¸å¿ƒçš„æ˜¯Document storesï¼ŒIndex storesï¼ŒVector stores



2. å‰4ç±»å‡æä¾›é»˜è®¤çš„æœ¬åœ°å­˜å‚¨ç±»ï¼Œ

   åœ¨æ„é€ StorageContextæ—¶ï¼Œè‹¥ä¸ä¼ å…¥å¯¹åº”çš„å¯¹è±¡ï¼Œç›´æ¥ä½¿ç”¨é»˜è®¤

   ```python
   docstore = docstore or SimpleDocumentStore()
   index_store = index_store or SimpleIndexStore()
   graph_store = graph_store or SimpleGraphStore()
   image_store = image_store or SimpleVectorStore()
   ```



3. LlamaIndex supports persisting data to any storage backend supported by [fsspec](https://filesystem-spec.readthedocs.io/en/latest/index.html). We have confirmed support for the following storage backends:

   - Local filesystem
   - AWS S3
   - Cloudflare R2

   ![img](images/LlamaIndex/storage.CHky3Ivr_2ghiKH.png)



4. **å­˜å‚¨éƒ¨åˆ†çš„æœ€é«˜ç®¡ç†å±‚çº§æ˜¯`StorageContext`**

   ```python
   from llama_index.core.storage.docstore import SimpleDocumentStore
   from llama_index.core.storage.index_store import SimpleIndexStore
   from llama_index.core.vector_stores import SimpleVectorStore
   from llama_index.core import StorageContext
   
   # create storage context using default stores
   storage_context = StorageContext.from_defaults(
       docstore=SimpleDocumentStore(),
       vector_store=SimpleVectorStore(),
       index_store=SimpleIndexStore(),
   )
   ```









## 4.1 Doc store

ä»¥å®˜æ–¹æ¨èçš„mongodbä¸ºä¾‹

1. å®‰è£…

   ```
   pip install llama-index-storage-docstore-mongodb
   ```

2. ä½¿ç”¨

   ```python
   from llama_index.storage.docstore.mongodb import MongoDocumentStore
   
   doc_mongodb_uri = f"mongodb://{user}:{password}@{host}:{port}/{database_name}?authSource=admin"
   document_store = MongoDocumentStore.from_uri(
       uri=doc_mongodb_uri,
       db_name=database_name,
       namespace="docstore"
   )
   ```

   æ³¨æ„ï¼š`namespace`æ˜¯ `database_name` ä¸‹çš„collectionçš„å‰ç¼€åï¼Œä¸æ•°æ®åº“æ— å…³ï¼Œå±äºllamaindexè‡ªå®šä¹‰

   ![image-20260222164754377](images/LlamaIndex/image-20260222164754377.png)

   

3. æ•°æ®ç¤ºä¾‹

   ï¼ˆ`metadata`ä¸ºç”¨æˆ·è‡ªå®šä¹‰å­—æ®µï¼‰

   mongodbä¸­çš„docstore/ref_doc_infoï¼šå½“å‰ä¸€å…±æœ‰å“ªäº›docï¼Œä»¥åŠå¯¹åº”node

   ```
   {
     "_id": "d057c30f-77aa-42f0-928d-eec27be5cdf7",
     "metadata": {
       "file_path": "rag/1.txt",
       "status": "active"
     },
     "node_ids": [
       "74b6d637-2dc4-4f76-a959-f55294cc559b",
       "1a1653a7-5de8-41b5-8db6-2f48febadb7b"
     ]
   }
   ```

   

   mongodbä¸­çš„docstore/metadataï¼šå½“å‰æœ‰å“ªäº›node

   ```
   {
     "_id": "d057c30f-77aa-42f0-928d-eec27be5cdf7",
     "doc_hash": "87f34d8b4fb8e5daf17ffd2ca5718e5c25659397d132c8288659b7d8c47f5cfc"
   }
   {
     "_id": "74b6d637-2dc4-4f76-a959-f55294cc559b",
     "doc_hash": "302368e7fd103a7c6febcd1a4dcc4cda1a4cf1ea623f28d2b6cdbb977d631697",
     "ref_doc_id": "d057c30f-77aa-42f0-928d-eec27be5cdf7"
   }
   {
     "_id": "1a1653a7-5de8-41b5-8db6-2f48febadb7b",
     "doc_hash": "26e690f4853bd364792b361f4ebb0178927e881bc6b211640db7213982851de6",
     "ref_doc_id": "d057c30f-77aa-42f0-928d-eec27be5cdf7"
   }
   ```

   

   mongodbä¸­çš„docstore/dataï¼šæ¯ä¸ªnodeå…·ä½“ä¿å­˜çš„ä¿¡æ¯

   ```
   {
     "_id": "74b6d637-2dc4-4f76-a959-f55294cc559b",
     "__data__": {
       "id_": "74b6d637-2dc4-4f76-a959-f55294cc559b",
       "embedding": null,
       "metadata": {
         "file_path": "rag/1.txt",
         "status": "processing"
       },
       "excluded_embed_metadata_keys": [
         "file_name",
         "file_type",
         "file_size",
         "creation_date",
         "last_modified_date",
         "last_accessed_date"
       ],
       "excluded_llm_metadata_keys": [
         "file_name",
         "file_type",
         "file_size",
         "creation_date",
         "last_modified_date",
         "last_accessed_date"
       ],
       "relationships": {
         "1": {
           "node_id": "d057c30f-77aa-42f0-928d-eec27be5cdf7",
           "node_type": "4",
           "metadata": {
             "file_path": "rag/1.txt",
             "status": "processing"
           },
           "hash": "87f34d8b4fb8e5daf17ffd2ca5718e5c25659397d132c8288659b7d8c47f5cfc",
           "class_name": "RelatedNodeInfo"
         },
         "3": {
           "node_id": "1a1653a7-5de8-41b5-8db6-2f48febadb7b",
           "node_type": "1",
           "metadata": {},
           "hash": "3e573ded2151ddfa478d2d551c9ed8992ee7a1f9f48595b72f7d1f6b3e4e4fa2",
           "class_name": "RelatedNodeInfo"
         }
       },
       "metadata_template": "{key}: {value}",
       "metadata_separator": "\n",
       "text": "å…³äºæ•æ·å¼€å‘ï¼ˆAgile Developmentï¼‰ç†è®ºï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸åºå¤§ä¸”ä¸æ–­å‘å±•çš„ä½“ç³»ã€‚å¦‚æœæˆ‘ä»¬è¦æ¢³ç†å®ƒçš„â€œä»£è¡¨ä½œâ€ï¼Œé€šå¸¸å¯ä»¥åˆ†ä¸ºå¥ åŸºç»å…¸ã€æ ¸å¿ƒæ¡†æ¶å’Œå·¥ç¨‹å®è·µä¸‰å¤§ç±»ã€‚\n\nåŸºäºæˆ‘ä¸ºä½ æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œä»¥ä¸‹æ˜¯è¯¥é¢†åŸŸæœ€å…·æƒå¨æ€§å’Œä»£è¡¨æ€§çš„ä¹¦ç±æ¸…å•ï¼š\n\nğŸ“š 1. å¥ åŸºä¸æ ¸å¿ƒç†è®ºï¼ˆå¿…è¯»ç»å…¸ï¼‰\nå¦‚æœä½ æƒ³ç†è§£æ•æ·çš„â€œä¸ºä»€ä¹ˆâ€å’Œâ€œæ˜¯ä»€ä¹ˆâ€ï¼Œè¿™ä¸¤æœ¬ä¹¦æ˜¯ç»•ä¸å¼€çš„åŸºçŸ³ï¼š\n\nã€Šæ•æ·è½¯ä»¶å¼€å‘ï¼šåŸåˆ™ã€æ¨¡å¼ä¸å®è·µã€‹ (Agile Software Development: Principles, Patterns, and Practices)\nä½œè€…ï¼š Robert C. Martin (Uncle Bob)\nåœ°ä½ï¼š è¢«èª‰ä¸ºæ•æ·å¼€å‘çš„â€œåœ£ç»â€ã€‚è¿™æœ¬ä¹¦ä¸ä»…é˜è¿°äº†æ•æ·çš„åŸåˆ™ï¼Œè¿˜ç»“åˆäº†é¢å‘å¯¹è±¡è®¾è®¡çš„åŸåˆ™ï¼ˆSOLIDåŸåˆ™ï¼‰ã€‚\næ ¸å¿ƒå†…å®¹ï¼š å®ƒæ·±å…¥æ¢è®¨äº†æ•æ·å¼€å‘çš„æ ¸å¿ƒç†å¿µã€è®¾è®¡æ¨¡å¼ä»¥åŠå®é™…ç¼–ç¨‹è§„èŒƒï¼Œå¼ºè°ƒäº†ä»£ç çš„æ•´æ´å’Œå¯ç»´æŠ¤æ€§ã€‚å¯¹äºå¼€å‘è€…å’Œç®¡ç†è€…æ¥è¯´ï¼Œè¿™æ˜¯ç†è§£æ•æ·å…¨è²Œçš„æœ€ä½³èµ·ç‚¹ã€‚\nã€Šæ•æ·å®£è¨€ã€‹ (Agile Manifesto)\næ³¨æ„ï¼š è™½ç„¶å®ƒä¸æ˜¯ä¸€æœ¬ä¹¦ï¼Œä½†å®ƒæ˜¯æ‰€æœ‰æ•æ·ç†è®ºçš„æºå¤´ã€‚ç”±Kent Beckç­‰17ä½è½¯ä»¶å¼€å‘é¢†å†›äººç‰©äº2001å¹´ç­¾ç½²ï¼Œç¡®ç«‹äº†â€œä¸ªä½“å’Œäº’åŠ¨ã€å¯å·¥ä½œçš„è½¯ä»¶ã€å®¢æˆ·åˆä½œã€å“åº”å˜åŒ–â€å››å¤§ä»·å€¼è§‚ã€‚\nğŸ—ï¸ 2. ä¸»æµæ¡†æ¶è¯¦è§£ï¼ˆScrumä¸çœ‹æ¿ï¼‰\nåœ¨å®é™…æ“ä½œä¸­ï¼Œå¤§å¤šæ•°å›¢é˜Ÿä½¿ç”¨çš„æ˜¯ Scrum æˆ– çœ‹æ¿ã€‚ä»¥ä¸‹æ˜¯è¿™ä¸¤ä¸ªé¢†åŸŸçš„æƒå¨è‘—ä½œï¼š\n\nã€ŠScrumæŒ‡å—ã€‹ (The Scrum Guide)\nä½œè€…ï¼š Ken Schwaber å’Œ Jeff Sutherland (Scrumçš„åˆ›å§‹äºº)\nåœ°ä½ï¼š è¿™æ˜¯Scrumçš„â€œå®˜æ–¹å®šä¹‰â€ï¼Œæ˜¯æœ€æƒå¨ã€æœ€ç²¾ç®€çš„å…¥é—¨å¿…è¯»ã€‚å®ƒå®šä¹‰äº†Scrumçš„è§’è‰²ï¼ˆå¦‚Scrum Masterã€äº§å“è´Ÿè´£äººï¼‰ã€äº‹ä»¶ï¼ˆSprintã€ç«™ä¼šï¼‰å’Œå·¥ä»¶ï¼ˆäº§å“å¾…åŠåˆ—è¡¨ï¼‰ã€‚\nã€ŠSCRUMï¼šç”¨ä¸¤å€é€Ÿåº¦åšä¸¤å€å·¥ä½œã€‹ (SCRUM: The Art of Doing Twice the Work in Half the Time)\nä½œè€…ï¼š Jeff Sutherland\næ ¸å¿ƒå†…å®¹ï¼š ä½œè€…ä½œä¸ºScrumåˆ›å§‹äººï¼Œé€šè¿‡ç”ŸåŠ¨çš„å•†ä¸šæ¡ˆä¾‹è®²è¿°äº†Scrumæ–¹æ³•çš„æ ¸å¿ƒåŸç†ã€‚ä¹¦ä¸­å¼ºè°ƒäº†è·¨èŒèƒ½å›¢é˜Ÿåä½œã€æ¯æ—¥ç«‹ä¼šä»¥åŠMVPï¼ˆæœ€å°å¯è¡Œäº§å“ï¼‰å¿«é€ŸéªŒè¯çš„ç†å¿µï¼Œä¸ä»…é€‚ç”¨äºè½¯ä»¶å¼€å‘ï¼Œè¿˜å»¶ä¼¸åˆ°äº†å…¶ä»–è¡Œä¸šã€‚\nã€Šçœ‹æ¿æ–¹æ³•ï¼šæŠ€æœ¯ä¼ä¸šçš„æ•æ·ç®¡ç†ã€‹ (Kanban: Successful Evolutionary Change for Your Technology Business)\nä½œè€…ï¼š David J. Anderson\næ ¸å¿ƒå†…å®¹ï¼š è¯¦ç»†ä»‹ç»äº†çœ‹æ¿æ–¹æ³•çš„åŸºæœ¬åŸç†å’Œå®è·µã€‚çœ‹æ¿å¼ºè°ƒå¯è§†åŒ–å·¥ä½œæµã€é™åˆ¶åœ¨åˆ¶å“æ•°é‡ï¼ˆWIPï¼‰ï¼Œéå¸¸é€‚åˆé‚£äº›æµç¨‹å˜æ›´é˜»åŠ›è¾ƒå¤§æˆ–ç»´æŠ¤å·¥ä½œè¾ƒå¤šçš„å›¢é˜Ÿã€‚\nğŸ’» 3.",
       "mimetype": "text/plain",
       "start_char_idx": 0,
       "end_char_idx": 1161,
       "metadata_seperator": "\n",
       "text_template": "{metadata_str}\n\n{content}",
       "class_name": "TextNode"
     },
     "__type__": "1"
   }
   ```

   

## 4.2 Vec store

- æ”¯æŒçš„å¤šç§æ•°æ®åº“ç±»å‹ä»¥åŠç‰¹æ€§å¯¹æ¯”ï¼š[Vector Store Options & Feature Support]([Vector Stores | LlamaIndex Python Documentation](https://developers.llamaindex.ai/python/framework/module_guides/storing/vector_stores/#vector-store-options--feature-support))

- æ­¤å¤„ä»‹ç»åŸºäºQdrantçš„ç”¨æ³•ï¼š[Qdrant Vector Store | LlamaIndex Python Documentation](https://developers.llamaindex.ai/python/framework/integrations/vector_stores/qdrantindexdemo/)

  

1. å®‰è£…æ’ä»¶ï¼ˆå¯ä» [llamahub](https://llamahub.ai/) ä¸­æ‰¾åˆ°ï¼š[LlamaIndex Vector_Stores Integration: Qdrant](https://llamahub.ai/l/vector_stores/llama-index-vector-stores-qdrant?from=vector_stores)ï¼‰

   ```
   pip install llama-index-vector-stores-qdrant
   pip install qdrant-client
   ```

   

2. åŒæ­¥ä½¿ç”¨

   ```python
   from llama_index.vector_stores.qdrant import QdrantVectorStore
   from qdrant_client import QdrantClient
   
   client = QdrantClient(
       host="<host>", 
       port="<port>"
   )
   vector_store = QdrantVectorStore(
       client=client, 
       collection_name=vector_config.collection_name
   )
   
   # NOTE: VERY IMPORTANT FOR THE INDEX STUCTURE CONSISTENCY
   vector_store.stores_text = False
   ```

   ==**è¦ç‰¹åˆ«æ³¨æ„è®¾ç½®vector_store.stores_text = Falseï¼Œå¦åˆ™ä¸SimpleXXXStore()ä¸­ä¿å­˜çš„æ•°æ®ç»“æ„ä¸ä¸€è‡´**==



3. å¼‚æ­¥ä½¿ç”¨

   ```python
   from llama_index.vector_stores.qdrant import QdrantVectorStore
   from qdrant_client import AsyncQdrantClient, QdrantClient
   
   client = QdrantClient(
       host="<host>", 
       port="<port>"
   )
   
   aclient = AsyncQdrantClient(
       host="<host>", 
       port="<port>"
   )
   
   vector_store = QdrantVectorStore(
       collection_name="paul_graham",
       client=client,							# æ³¨æ„è¦ä¼ ä¸¤ä¸ª
       aclient=aclient							# æ³¨æ„è¦ä¼ ä¸¤ä¸ª
   )
   
   storage_context = StorageContext.from_defaults(vector_store=vector_store)
   index = VectorStoreIndex.from_documents(
       documents,
       storage_context=storage_context,
       use_async=True,							# æ³¨æ„
   )
   ```



4. å•æ¡æ•°æ®payloadç¤ºä¾‹ï¼š

   ```json
   {"file_path":"rag/1.txt","status":"processing","_node_content":"{\"id_\": \"74b6d637-2dc4-4f76-a959-f55294cc559b\", \"embedding\": null, \"metadata\": {\"file_path\": \"rag/1.txt\", \"status\": \"processing\"}, \"excluded_embed_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"excluded_llm_metadata_keys\": [\"file_name\", \"file_type\", \"file_size\", \"creation_date\", \"last_modified_date\", \"last_accessed_date\"], \"relationships\": {\"1\": {\"node_id\": \"d057c30f-77aa-42f0-928d-eec27be5cdf7\", \"node_type\": \"4\", \"metadata\": {\"file_path\": \"rag/1.txt\", \"status\": \"processing\"}, \"hash\": \"87f34d8b4fb8e5daf17ffd2ca5718e5c25659397d132c8288659b7d8c47f5cfc\", \"class_name\": \"RelatedNodeInfo\"}, \"3\": {\"node_id\": \"1a1653a7-5de8-41b5-8db6-2f48febadb7b\", \"node_type\": \"1\", \"metadata\": {}, \"hash\": \"3e573ded2151ddfa478d2d551c9ed8992ee7a1f9f48595b72f7d1f6b3e4e4fa2\", \"class_name\": \"RelatedNodeInfo\"}}, \"metadata_template\": \"{key}: {value}\", \"metadata_separator\": \"\\n\", \"text\": \"å…³äºæ•æ·å¼€å‘ï¼ˆAgile Developmentï¼‰ç†è®ºï¼Œè¿™æ˜¯ä¸€ä¸ªéå¸¸åºå¤§ä¸”ä¸æ–­å‘å±•çš„ä½“ç³»ã€‚å¦‚æœæˆ‘ä»¬è¦æ¢³ç†å®ƒçš„â€œä»£è¡¨ä½œâ€ï¼Œé€šå¸¸å¯ä»¥åˆ†ä¸ºå¥ åŸºç»å…¸ã€æ ¸å¿ƒæ¡†æ¶å’Œå·¥ç¨‹å®è·µä¸‰å¤§ç±»ã€‚\\n\\nåŸºäºæˆ‘ä¸ºä½ æ£€ç´¢åˆ°çš„ä¿¡æ¯ï¼Œä»¥ä¸‹æ˜¯è¯¥é¢†åŸŸæœ€å…·æƒå¨æ€§å’Œä»£è¡¨æ€§çš„ä¹¦ç±æ¸…å•ï¼š\\n\\nğŸ“š 1. å¥ åŸºä¸æ ¸å¿ƒç†è®ºï¼ˆå¿…è¯»ç»å…¸ï¼‰\\nå¦‚æœä½ æƒ³ç†è§£æ•æ·çš„â€œä¸ºä»€ä¹ˆâ€å’Œâ€œæ˜¯ä»€ä¹ˆâ€ï¼Œè¿™ä¸¤æœ¬ä¹¦æ˜¯ç»•ä¸å¼€çš„åŸºçŸ³ï¼š\\n\\nã€Šæ•æ·è½¯ä»¶å¼€å‘ï¼šåŸåˆ™ã€æ¨¡å¼ä¸å®è·µã€‹ (Agile Software Development: Principles, Patterns, and Practices)\\nä½œè€…ï¼š Robert C. Martin (Uncle Bob)\\nåœ°ä½ï¼š è¢«èª‰ä¸ºæ•æ·å¼€å‘çš„â€œåœ£ç»â€ã€‚è¿™æœ¬ä¹¦ä¸ä»…é˜è¿°äº†æ•æ·çš„åŸåˆ™ï¼Œè¿˜ç»“åˆäº†é¢å‘å¯¹è±¡è®¾è®¡çš„åŸåˆ™ï¼ˆSOLIDåŸåˆ™ï¼‰ã€‚\\næ ¸å¿ƒå†…å®¹ï¼š å®ƒæ·±å…¥æ¢è®¨äº†æ•æ·å¼€å‘çš„æ ¸å¿ƒç†å¿µã€è®¾è®¡æ¨¡å¼ä»¥åŠå®é™…ç¼–ç¨‹è§„èŒƒï¼Œå¼ºè°ƒäº†ä»£ç çš„æ•´æ´å’Œå¯ç»´æŠ¤æ€§ã€‚å¯¹äºå¼€å‘è€…å’Œç®¡ç†è€…æ¥è¯´ï¼Œè¿™æ˜¯ç†è§£æ•æ·å…¨è²Œçš„æœ€ä½³èµ·ç‚¹ã€‚\\nã€Šæ•æ·å®£è¨€ã€‹ (Agile Manifesto)\\næ³¨æ„ï¼š è™½ç„¶å®ƒä¸æ˜¯ä¸€æœ¬ä¹¦ï¼Œä½†å®ƒæ˜¯æ‰€æœ‰æ•æ·ç†è®ºçš„æºå¤´ã€‚ç”±Kent Beckç­‰17ä½è½¯ä»¶å¼€å‘é¢†å†›äººç‰©äº2001å¹´ç­¾ç½²ï¼Œç¡®ç«‹äº†â€œä¸ªä½“å’Œäº’åŠ¨ã€å¯å·¥ä½œçš„è½¯ä»¶ã€å®¢æˆ·åˆä½œã€å“åº”å˜åŒ–â€å››å¤§ä»·å€¼è§‚ã€‚\\nğŸ—ï¸ 2. ä¸»æµæ¡†æ¶è¯¦è§£ï¼ˆScrumä¸çœ‹æ¿ï¼‰\\nåœ¨å®é™…æ“ä½œä¸­ï¼Œå¤§å¤šæ•°å›¢é˜Ÿä½¿ç”¨çš„æ˜¯ Scrum æˆ– çœ‹æ¿ã€‚ä»¥ä¸‹æ˜¯è¿™ä¸¤ä¸ªé¢†åŸŸçš„æƒå¨è‘—ä½œï¼š\\n\\nã€ŠScrumæŒ‡å—ã€‹ (The Scrum Guide)\\nä½œè€…ï¼š Ken Schwaber å’Œ Jeff Sutherland (Scrumçš„åˆ›å§‹äºº)\\nåœ°ä½ï¼š è¿™æ˜¯Scrumçš„â€œå®˜æ–¹å®šä¹‰â€ï¼Œæ˜¯æœ€æƒå¨ã€æœ€ç²¾ç®€çš„å…¥é—¨å¿…è¯»ã€‚å®ƒå®šä¹‰äº†Scrumçš„è§’è‰²ï¼ˆå¦‚Scrum Masterã€äº§å“è´Ÿè´£äººï¼‰ã€äº‹ä»¶ï¼ˆSprintã€ç«™ä¼šï¼‰å’Œå·¥ä»¶ï¼ˆäº§å“å¾…åŠåˆ—è¡¨ï¼‰ã€‚\\nã€ŠSCRUMï¼šç”¨ä¸¤å€é€Ÿåº¦åšä¸¤å€å·¥ä½œã€‹ (SCRUM: The Art of Doing Twice the Work in Half the Time)\\nä½œè€…ï¼š Jeff Sutherland\\næ ¸å¿ƒå†…å®¹ï¼š ä½œè€…ä½œä¸ºScrumåˆ›å§‹äººï¼Œé€šè¿‡ç”ŸåŠ¨çš„å•†ä¸šæ¡ˆä¾‹è®²è¿°äº†Scrumæ–¹æ³•çš„æ ¸å¿ƒåŸç†ã€‚ä¹¦ä¸­å¼ºè°ƒäº†è·¨èŒèƒ½å›¢é˜Ÿåä½œã€æ¯æ—¥ç«‹ä¼šä»¥åŠMVPï¼ˆæœ€å°å¯è¡Œäº§å“ï¼‰å¿«é€ŸéªŒè¯çš„ç†å¿µï¼Œä¸ä»…é€‚ç”¨äºè½¯ä»¶å¼€å‘ï¼Œè¿˜å»¶ä¼¸åˆ°äº†å…¶ä»–è¡Œä¸šã€‚\\nã€Šçœ‹æ¿æ–¹æ³•ï¼šæŠ€æœ¯ä¼ä¸šçš„æ•æ·ç®¡ç†ã€‹ (Kanban: Successful Evolutionary Change for Your Technology Business)\\nä½œè€…ï¼š David J. Anderson\\næ ¸å¿ƒå†…å®¹ï¼š è¯¦ç»†ä»‹ç»äº†çœ‹æ¿æ–¹æ³•çš„åŸºæœ¬åŸç†å’Œå®è·µã€‚çœ‹æ¿å¼ºè°ƒå¯è§†åŒ–å·¥ä½œæµã€é™åˆ¶åœ¨åˆ¶å“æ•°é‡ï¼ˆWIPï¼‰ï¼Œéå¸¸é€‚åˆé‚£äº›æµç¨‹å˜æ›´é˜»åŠ›è¾ƒå¤§æˆ–ç»´æŠ¤å·¥ä½œè¾ƒå¤šçš„å›¢é˜Ÿã€‚\\nğŸ’» 3.\", \"mimetype\": \"text/plain\", \"start_char_idx\": 0, \"end_char_idx\": 1161, \"metadata_seperator\": \"\\n\", \"text_template\": \"{metadata_str}\\n\\n{content}\", \"class_name\": \"TextNode\"}","_node_type":"TextNode","document_id":"d057c30f-77aa-42f0-928d-eec27be5cdf7","doc_id":"d057c30f-77aa-42f0-928d-eec27be5cdf7","ref_doc_id":"d057c30f-77aa-42f0-928d-eec27be5cdf7"}
   ```

   

## 4.3 Index store

ä»¥å®˜æ–¹æ¨èçš„mongodbä¸ºä¾‹

1. å®‰è£…

   ```
   pip install llama-index-storage-index-store-mongodb
   ```

2. ä½¿ç”¨

   ```python
   from llama_index.storage.index_store.mongodb import MongoIndexStore
   from llama_index.core import VectorStoreIndex
   
   doc_mongodb_uri = f"mongodb://{user}:{password}@{host}:{port}/{database_name}?authSource=admin"
   
   index_store = MongoIndexStore.from_uri(
       uri=index_mongodb_uri,
       db_name=index_config.database,
       namespace="indexstore"
   )
   
   # create storage context
   storage_context = StorageContext.from_defaults(index_store=index_store)
   
   # build index
   index = VectorStoreIndex(nodes, storage_context=storage_context)
   ```

3. æ•°æ®ç¤ºä¾‹

   mongodbä¸­çš„indexstoreï¼šæ˜¯æŒ‡å½“å‰indexä¸­æœ‰å“ªäº›node

   ```json
   {
     "_id": "d67eebb0-d05d-4c19-a93a-99a0db4961aa",
     "__data__": "{\"index_id\": \"d67eebb0-d05d-4c19-a93a-99a0db4961aa\", \"summary\": null, \"nodes_dict\": {\"74b6d637-2dc4-4f76-a959-f55294cc559b\": \"74b6d637-2dc4-4f76-a959-f55294cc559b\", \"1a1653a7-5de8-41b5-8db6-2f48febadb7b\": \"1a1653a7-5de8-41b5-8db6-2f48febadb7b\"}, \"doc_id_dict\": {}, \"embeddings_dict\": {}}",
     "__type__": "vector_store"
   }
   ```

   

# 5. Indexing

- å…³äºIndexçš„ä¸åŒç§ç±»ï¼š[How Each Index Works]([How Each Index Works | LlamaIndex Python Documentation](https://developers.llamaindex.ai/python/framework/module_guides/indexing/index_guide/))

- æ­¤å¤„åªè®¨è®ºæœ€ç¬¦åˆRAGç†å¿µçš„[Vector Store Index](https://developers.llamaindex.ai/python/framework/module_guides/indexing/index_guide/#vector-store-index)

- æ­¤å¤– [Tree Index](https://developers.llamaindex.ai/python/framework/module_guides/indexing/index_guide/#tree-index) å’Œ [Property Graph Index](https://developers.llamaindex.ai/python/framework/module_guides/indexing/index_guide/#property-graph-index) ï¼ˆçŸ¥è¯†å›¾è°±ï¼‰å€¼å¾—å…³æ³¨



## 5.1 åŸºç¡€ä½¿ç”¨

1. ä½¿ç”¨

   ```python
   from llama_index.core import VectorStoreIndex
   
   # æ„å»ºç´¢å¼•
   index = VectorStoreIndex.from_documents(
       documents,
       storage_context=storage_context
       # transformations=...
   )
   
   # åˆ›å»ºå¼•æ“
   retriever = index.as_retriever(similarity_top_k=1)
   query_engine = index.as_query_engine()
   
   # ä½¿ç”¨
   result = retriever.retrieve("hello")
   result = query_engine.query("åº”è¯¥æ¨èé‚£ä¸¤æœ¬ä¹¦ï¼Ÿ")
   ```

   

## 5.2 index å¸¸ç”¨æ“ä½œ

1. insert

   > You can â€œinsertâ€ a new Document into any index data structure, after building the index initially. This document will be broken down into nodes and ingested into the index.
   >
   > For the vector store index, a new Document (and embeddings) is inserted into the underlying document/embedding store.

   ```python
   index.insert(document=doc)
   ```

   

2. Deletion

   > You can â€œdeleteâ€ a Document from most index data structures by specifying a document_id. (**NOTE**: the tree index currently does not support deletion). All nodes corresponding to the document will be deleted.

   ```
   index.delete_ref_doc("doc_id_0", delete_from_docstore=True)
   ```

   

   

3. Document Tracking

   > Any index that uses the docstore (i.e. all indexes except for most vector store integrations), you can also see which documents you have inserted into the docstore.

   ```python
   print(index.ref_doc_info)
   """
   > {'doc_id_1': RefDocInfo(node_ids=['071a66a8-3c47-49ad-84fa-7010c6277479'], metadata={}),
      'doc_id_2': RefDocInfo(node_ids=['9563e84b-f934-41c3-acfd-22e88492c869'], metadata={}),
      'doc_id_0': RefDocInfo(node_ids=['b53e6c2f-16f7-4024-af4c-42890e945f36'], metadata={}),
      'doc_id_3': RefDocInfo(node_ids=['6bedb29f-15db-4c7c-9885-7490e10aa33f'], metadata={})}
   """
   ```

   

4. æ³¨æ„ï¼šinsert å’Œ delete æ“ä½œä¸èƒ½ä¿è¯å¤šæ•°æ®åº“çš„ä¸€è‡´æ€§

   ç‰¹åˆ«æ˜¯ delete æ“ä½œï¼Œå¹¶å‘æ—¶å°†å¯¼è‡´é—®é¢˜
   
   ```python
   # llama_index/core/indices/base.py
   class BaseIndex(Generic[IS], ABC):
               
       def insert(self, document: Document, **insert_kwargs: Any) -> None:
           """Insert a document."""
           with self._callback_manager.as_trace("insert"):
               nodes = run_transformations(
                   [document],
                   self._transformations,
                   show_progress=self._show_progress,
                   **insert_kwargs,
               )
   
               self.insert_nodes(nodes, **insert_kwargs)
               self.docstore.set_document_hash(document.id_, document.hash)
   ```
   
   ```python
   # llama_index/core/indices/vector_store/base.py
   class VectorStoreIndex(BaseIndex[IndexDict]):
       
       def insert_nodes(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:
           """
           Insert nodes.
   
           NOTE: overrides BaseIndex.insert_nodes.
               VectorStoreIndex only stores nodes in document store
               if vector store does not store text
           """
           self._validate_serializable(nodes)
   
           with self._callback_manager.as_trace("insert_nodes"):
               self._insert(nodes, **insert_kwargs)
               self._storage_context.index_store.add_index_struct(self._index_struct)
               
       def _insert(self, nodes: Sequence[BaseNode], **insert_kwargs: Any) -> None:
           """Insert a document."""
           self._add_nodes_to_index(self._index_struct, nodes, **insert_kwargs)
           
       def _add_nodes_to_index(
           self,
           index_struct: IndexDict,
           nodes: Sequence[BaseNode],
           show_progress: bool = False,
           **insert_kwargs: Any,
       ) -> None:
           """Add document to index."""
           if not nodes:
               return
   
           for nodes_batch in iter_batch(nodes, self._insert_batch_size):
               nodes_batch = self._get_node_with_embedding(nodes_batch, show_progress)
               new_ids = self._vector_store.add(nodes_batch, **insert_kwargs)
   
               if not self._vector_store.stores_text or self._store_nodes_override:
                   # NOTE: if the vector store doesn't store text,
                   # we need to add the nodes to the index struct and document store
                   for node, new_id in zip(nodes_batch, new_ids):
                       # NOTE: remove embedding from node to avoid duplication
                       node_without_embedding = node.model_copy()
                       node_without_embedding.embedding = None
   
                       index_struct.add_node(node_without_embedding, text_id=new_id)
                       self._docstore.add_documents(
                           [node_without_embedding], allow_update=True
                       )
               else:
                   # NOTE: if the vector store keeps text,
                   # we only need to add image and index nodes
                   for node, new_id in zip(nodes_batch, new_ids):
                       if isinstance(node, (ImageNode, IndexNode)):
                           # NOTE: remove embedding from node to avoid duplication
                           node_without_embedding = node.model_copy()
                           node_without_embedding.embedding = None
   
                           index_struct.add_node(node_without_embedding, text_id=new_id)
                           self._docstore.add_documents(
                               [node_without_embedding], allow_update=True
                           )
                           
       def delete_ref_doc(
           self, ref_doc_id: str, delete_from_docstore: bool = False, **delete_kwargs: Any
       ) -> None:
           """Delete a document and it's nodes by using ref_doc_id."""
           self._vector_store.delete(ref_doc_id, **delete_kwargs)
           self._delete_from_index_struct(ref_doc_id)
           if delete_from_docstore:
               self._delete_from_docstore(ref_doc_id)
           self._storage_context.index_store.add_index_struct(self._index_struct)
   ```